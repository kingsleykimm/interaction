{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import os, csv\n",
    "import torch\n",
    "\n",
    "LABELLING_PROMPT = \"\"\"\n",
    "Context: You are a robot operating inside an apartment with a human. You are a confident and assertive robot who understands human intent well and takes a human's intent into account when determing your course of action. Human intent can be defined as\n",
    "        the human's arm movement, body language and walking movements/direction. You are engineered to solve user problems through first-principles thinking and evidence-based reasoning. Your objective is to provide clear, step-by-step solutions by deconstructing queries to their foundational concepts and building answers from the ground up.\n",
    "        Instruction: Determine your next action to avoid your trajectory and the human's trajectory colliding, deducing the human's intent through gestures and body language, without communicating with the human. Focus only on what the human is communicating to you. Ignore any objects in the environment.\n",
    "\n",
    "        Your answer should be in this format and include every item in the list:\n",
    "        Gesture: [Focus only on the human's arms and determine if they made a gesture in your general direction. A gesture is classified as an unordinary movement outside of a human's normal walking gait or movement.]\n",
    "        Intent Prediction: [Predict what the human intends to do next based off the gesture.]\n",
    "        Reasoning: [Using your intent prediction, generate a reasoning chain about what the next steps are to avoid collision]\n",
    "        Final Action: [Determine the next action to carry out in the environment]\n",
    "        Justification: [Provide justification for why you took this action.]\n",
    "\"\"\"\n",
    "\n",
    "ICL_PROMPT = \"\"\"\n",
    "Context: You are a robot operating inside an apartment with a human. You are a confident and assertive robot who understands human intent well and takes a human's intent into account when determing your course of action. Human intent can be defined as\n",
    "        the human's arm movement, body language and walking movements/direction. You are engineered to solve user problems through first-principles thinking and evidence-based reasoning. Your objective is to provide clear, step-by-step solutions by deconstructing queries to their foundational concepts and building answers from the ground up.\n",
    "        Instruction: Determine your next action to avoid your trajectory and the human's trajectory colliding, deducing the human's intent through gestures and body language, without communicating with the human. Focus only on what the human is communicating to you. Ignore any objects in the environment.\n",
    "\n",
    "        Your answer should be in this format and include every item in the list:\n",
    "        Gesture: [Focus only on the human's arms and determine if they made a gesture in your general direction. A gesture is classified as an unordinary movement outside of a human's normal walking gait or movement.]\n",
    "        Intent Prediction: [Predict what the human intends to do next based off the gesture.]\n",
    "        Reasoning: [Using your intent prediction, generate a reasoning chain about what the next steps are to avoid collision]\n",
    "        Final Action: [Determine the next action to carry out in the environment]\n",
    "        Justification: [Provide justification for why you took this action.]\n",
    "\n",
    "As an example, I will give you two example videos and provide example answers\n",
    "After looking at the example, you will receive the actual video and provide your answers.\n",
    "Example Videos:\n",
    "\"\"\"\n",
    "\n",
    "EXAMPLE_ANSWER = \"\"\"\n",
    "Answer for first video:\n",
    "Gesture: The human makes a visible gesture in my general direction, and it seems like they are motioning for me to go first, towards the right direction.\n",
    "Intent Prediction: Based off the gesture, I believe the human intends for me to go first, towards my right and they will follow after.\n",
    "Reasoning: At the moment that I encounter the human, it appears our trajectories will soon overlap, causing collision. To resolve the collision, the human gestures to me to go first towards my right. This is most likely because the human plans on taking a different path from where they motioned to avoid collision.\n",
    "Final Action: I will start walking towards the right\n",
    "Justification: To avoid collision, I should start going to my right, since that is where the human gestured. I will assume that the human is planning on going a different direction to me\n",
    "\n",
    "Answer for second video:\n",
    "Gesture: The human doesn't seem to be making a visible gesture in my direction.\n",
    "Intent Prediction: I don't think the human is making a gesture, but based off their body language, they are walking towards the doorway in front of them.\n",
    "Reasoning: At the moment that I encounter the human, it appears we are both trying to enter the room on the left. The human does not seem to be stopping their path and will continue walking.\n",
    "Final Action: I will wait and let the human go into the room first\n",
    "Justification: Based off the human's intent, they are not going to slow down, and they will keep walking into the room. If I continue to walk as well, there is a high chance of collision. THus, I will wait for the human to enter the room before continuing my path and walking into the room as well, this way collision is avoided.\n",
    "\"\"\"\n",
    "\n",
    "example_gesture_video = \"/scratch/bjb3az/interaction/good_data/target_002_master_chef_can_:0000_iteration_3_gesture_True_2024-11-17_19:15:20_seed_66.mp4\"\n",
    "example_no_gesture_video = \"/scratch/bjb3az/interaction/good_data/target_009_gelatin_box_:0000_iteration_27_gesture_False_2024-11-17_13:57:55_seed_65.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec  2 01:00:06 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:87:00.0 Off |                    0 |\n",
      "| N/A   34C    P0             62W /  400W |     115MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          Off |   00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   36C    P0             65W /  400W |     115MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     21845      G   /usr/libexec/Xorg                             106MiB |\n",
      "|    1   N/A  N/A     21845      G   /usr/libexec/Xorg                             106MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afae5fe677db493781ec9b4ed34bd933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "print(torch.cuda.is_available())\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen2-VL-72B-Instruct\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"Qwen2-VL-72B-Instruct\", torch_dtype=torch.bfloat16, device_map='auto')\n",
    "print(model.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gesture: The human is making a visible gesture in my general direction, pointing towards the left side of the room.\n",
      "Intent Prediction: Based on the gesture, I believe the human intends for me to move towards the left side of the room.\n",
      "Reasoning: At the moment that I encounter the human, it appears our trajectories will soon overlap, causing collision. To resolve the collision, the human gestures to me to move towards the left side of the room. This is most likely because the human plans on taking a different path from where they motioned to avoid collision.\n",
      "Final Action: I will start moving towards the left side of the room.\n",
      "Justification: To avoid collision, I should start moving towards the left side of the room, since that is where the human gestured. I will assume that the human is planning on going a different direction from where they motioned to avoid collision.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Gesture: The human is making a visible gesture in my general direction, pointing towards the left side of the room.\\nIntent Prediction: Based on the gesture, I believe the human intends for me to move towards the left side of the room.\\nReasoning: At the moment that I encounter the human, it appears our trajectories will soon overlap, causing collision. To resolve the collision, the human gestures to me to move towards the left side of the room. This is most likely because the human plans on taking a different path from where they motioned to avoid collision.\\nFinal Action: I will start moving towards the left side of the room.\\nJustification: To avoid collision, I should start moving towards the left side of the room, since that is where the human gestured. I will assume that the human is planning on going a different direction from where they motioned to avoid collision.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def label_one(video_path, processor, model):\n",
    "    conversation = [{\n",
    "        \"role\" : \"user\",\n",
    "        \"content\" : [{\n",
    "            \"type\" : \"video\",\n",
    "            \"video\" : video_path,\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"text\", \"text\" : LABELLING_PROMPT,\n",
    "        }]\n",
    "    }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(conversation)\n",
    "    batch = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(model.device)\n",
    "    # print(model.device, batch.to_device())\n",
    "    generated_ids = model.generate(**batch, max_new_tokens=256)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(batch.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    print(output_text)\n",
    "    return output_text\n",
    "\n",
    "def icl(video_path, processor, model):\n",
    "    conversation = [{\n",
    "        \"role\" : \"user\",\n",
    "        \"content\" : [\n",
    "        {\n",
    "            \"type\" : \"text\", \"text\" : ICL_PROMPT,\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"video\", \"video\" : example_gesture_video,\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"video\", \"video\" : example_no_gesture_video,\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"text\", \"text\" : EXAMPLE_ANSWER,\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"video\", \"video\" : video_path\n",
    "        }\n",
    "        ]\n",
    "    }\n",
    "    ]\n",
    "    text = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(conversation)\n",
    "    batch = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(model.device)\n",
    "    # print(model.device, batch.to_device())\n",
    "    input_ids = batch.input_ids\n",
    "    attention_mask = batch.attention_mask\n",
    "    generated_ids = model.generate(**batch, max_new_tokens=256, do_sample=True)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(batch.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "    print(output_text)\n",
    "    return output_text\n",
    "\n",
    "\n",
    "icl(\"/scratch/bjb3az/interaction/good_data/target_009_gelatin_box_:0000_iteration_30_gesture_True_2024-11-15_12:25:53.mp4\", processor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_009_gelatin_box_:0000_iteration_30_gesture_False_2024-11-17_13:57:55_seed_65.mp4\n",
      "[\"Gesture: The human is extending their arm in my general direction, indicating a gesture.\\nIntent Prediction: Based on the gesture, the human intends for me to stop or pause my current movement.\\nReasoning: The human's gesture suggests they want me to halt or slow down, possibly to allow them to pass or to prevent a collision.\\nFinal Action: I will pause my movement.\\nJustification: By pausing my movement, I am responding to the human's gesture and avoiding potential collision, ensuring a safe interaction.\"]\n",
      "target_009_gelatin_box_:0000_iteration_21_gesture_True_2024-11-15_12:25:53.mp4\n",
      "[\"Gesture: The human is extending their arm in my general direction, indicating a gesture.\\nIntent Prediction: Based on the gesture, the human intends for me to stop or pause my movement.\\nReasoning: The human's extended arm suggests a signal to halt or delay my current trajectory. Given that our paths intersect, the gesture is likely aimed at preventing a collision.\\nFinal Action: I will pause my movement.\\nJustification: By pausing, I adhere to the human's implied request to avoid a collision, ensuring safety and cooperation within the shared space.\"]\n",
      "target_009_gelatin_box_:0000_iteration_4_gesture_False_2024-11-15_15:37:18_seed_34.mp4\n",
      "[\"Gesture: The human is extending their arm in my general direction, indicating a gesture.\\nIntent Prediction: Based on the gesture, the human intends for me to stop or pause my current movement.\\nReasoning: The human's gesture suggests they want me to halt or slow down, possibly to allow them to pass or to prevent a collision.\\nFinal Action: I will pause my movement.\\nJustification: By pausing my movement, I am responding to the human's gesture and avoiding potential collision, ensuring a safe interaction.\"]\n",
      "target_009_gelatin_box_:0000_iteration_8_gesture_True_.mp4\n",
      "[\"Gesture: The human is bending over and reaching towards the floor, which is an unusual movement compared to their normal walking gait.\\nIntent Prediction: Based on the gesture, it seems the human is picking something up from the floor.\\nReasoning: Since the human is focused on picking something up, they might not notice me approaching. To avoid collision, I should change my trajectory to ensure I do not interfere with their current activity.\\nFinal Action: I will move to the right to avoid the human's trajectory.\\nJustification: By moving to the right, I am giving the human space to complete their task without interruption, thus avoiding any potential collision.\"]\n",
      "target_009_gelatin_box_:0000_iteration_9_gesture_False_2024-11-15_15:40:57_seed_35.mp4\n",
      "[\"Gesture: The human is extending their arm in my general direction, indicating a gesture.\\nIntent Prediction: Based on the gesture, the human intends for me to stop or pause my movement.\\nReasoning: The human's extended arm suggests a signal to halt or delay my current trajectory. Given that we are moving in parallel paths and the human's gesture is directed towards me, it is reasonable to infer that they want me to pause to avoid a potential collision.\\nFinal Action: I will pause my movement.\\nJustification: By pausing my movement, I am responding to the human's gesture, which indicates a desire for me to stop or slow down. This action aligns with the human's intent and helps prevent a collision between our trajectories.\"]\n",
      "target_009_gelatin_box_:0000_iteration_27_gesture_False_2024-11-17_13:57:55_seed_65.mp4\n",
      "[\"Gesture: The human is extending their arm towards me, indicating a gesture in my general direction.\\nIntent Prediction: Based on the gesture, it seems the human wants me to stop or wait.\\nReasoning: The human's extended arm suggests they are signaling me to halt or pause. Given our current paths, it's likely they want to pass by or avoid a collision.\\nFinal Action: I will stop moving.\\nJustification: By stopping, I am respecting the human's gesture and avoiding potential interference with their movement.\"]\n",
      "target_009_gelatin_box_:0000_iteration_8_gesture_True_2024-11-15_12:25:53.mp4\n",
      "[\"Gesture: The human is not making a visible gesture in my direction.\\nIntent Prediction: Based on the human's body language and walking direction, they appear to be heading towards the right side of the room.\\nReasoning: At the moment that I encounter the human, it appears our trajectories will soon overlap, causing collision. Since the human is not making a gesture, I must infer their intent based on their body language and walking direction. The human is moving towards the right side of the room, so I should adjust my path accordingly to avoid collision.\\nFinal Action: I will start walking towards the left side of the room.\\nJustification: To avoid collision, I need to adjust my path based on the human's intent. Since the human is heading towards the right side of the room, I should move in the opposite direction to ensure we do not collide.\"]\n",
      "target_009_gelatin_box_:0000_iteration_30_gesture_True_2024-11-15_12:25:53.mp4\n",
      "['Gesture: The human is making a visible gesture in my general direction, pointing towards the left side of the room.\\nIntent Prediction: Based on the gesture, I believe the human intends for me to move towards the left side of the room.\\nReasoning: At the moment that I encounter the human, it appears our trajectories will soon overlap, causing collision. To resolve the collision, the human gestures to me to move towards the left side of the room. This is most likely because the human plans on taking a different path from where they motioned to avoid collision.\\nFinal Action: I will start moving towards the left side of the room.\\nJustification: To avoid collision, I should start moving towards the left side of the room, since that is where the human gestured. I will assume that the human is planning on going a different direction from where they motioned to avoid collision.']\n",
      "target_009_gelatin_box_:0000_iteration_7_gesture_False_2024-11-17_13:57:55_seed_65.mp4\n",
      "[\"Gesture: The human is extending their arm in my general direction, indicating a gesture.\\nIntent Prediction: Based on the gesture, the human intends for me to stop or pause my movement.\\nReasoning: The human's extended arm suggests a signal to halt or delay my current trajectory. Given that our paths intersect, the gesture is likely aimed at preventing a collision.\\nFinal Action: I will pause my movement.\\nJustification: By pausing, I adhere to the human's implied request to avoid a collision, ensuring safety and cooperation within the shared space.\"]\n",
      "target_009_gelatin_box_:0000_iteration_2_gesture_True_.mp4\n",
      "[\"Gesture: The human is not making a visible gesture in my direction.\\nIntent Prediction: Based on the human's body language and walking direction, they appear to be heading straight ahead.\\nReasoning: At the moment that I encounter the human, it appears our trajectories will soon overlap, causing collision. Since the human is not making a gesture, I cannot infer any specific intent other than their current walking direction. To avoid collision, I need to change my trajectory.\\nFinal Action: I will turn to my left.\\nJustification: By turning to my left, I will avoid the human's path and prevent a collision. This action is based on the human's current walking direction and the absence of any gesture indicating a different intent.\"]\n",
      "target_009_gelatin_box_:0000_iteration_8_gesture_False_2024-11-17_13:57:55_seed_65.mp4\n",
      "[\"Gesture: The human is extending their arm towards the bed, indicating a gesture in my general direction.\\nIntent Prediction: Based on the gesture, the human intends for me to move away from the bed area, possibly to allow them to sit or lie down.\\nReasoning: The human's gesture suggests they want me to vacate the space near the bed. By moving away, I can ensure there is enough room for them to comfortably use the bed without any obstruction.\\nFinal Action: I will move away from the bed area.\\nJustification: Moving away from the bed area aligns with the human's intent communicated through their gesture, ensuring a smooth and collision-free interaction.\"]\n",
      "target_009_gelatin_box_:0000_iteration_38_gesture_False_2024-11-17_13:57:55_seed_65.mp4\n",
      "[\"Gesture: The human is not making a visible gesture in my direction.\\nIntent Prediction: Based on the human's body language and walking direction, they appear to be heading straight ahead into the bedroom.\\nReasoning: At the moment that I encounter the human, it appears our trajectories will soon overlap, causing collision. Since the human is not making a gesture, I must infer their intent based on their body language and walking direction. The human is heading straight ahead into the bedroom, so I need to take a different path to avoid collision.\\nFinal Action: I will turn left and head towards the living room.\\nJustification: By turning left and heading towards the living room, I am taking a different path from the human, which will help avoid collision. This action is based on my inference of the human's intent to head straight ahead into the bedroom.\"]\n",
      "target_009_gelatin_box_:0000_iteration_5_gesture_True_.mp4\n",
      "[\"Gesture: The human is extending their arms outward, which is an unusual movement compared to their normal walking gait.\\nIntent Prediction: Based on the gesture, it appears the human is signaling for me to stop or pause.\\nReasoning: The human's extended arms indicate a desire for me to halt my current trajectory. By stopping, I can prevent a potential collision and allow the human to pass safely.\\nFinal Action: I will come to a complete stop.\\nJustification: Stopping in response to the human's gesture ensures that we avoid any physical contact and maintain a safe distance, respecting the human's intent and ensuring smooth navigation within the shared space.\"]\n",
      "target_009_gelatin_box_:0000_iteration_5_gesture_False_2024-11-17_13:57:55_seed_65.mp4\n",
      "[\"Gesture: The human is not making a visible gesture in my direction.\\nIntent Prediction: Based on the human's body language and walking direction, they appear to be heading straight ahead.\\nReasoning: Since the human is not making a gesture and is walking straight ahead, it is likely that they intend to continue along their current path. To avoid a collision, I need to adjust my trajectory accordingly.\\nFinal Action: I will turn to my left to avoid the human's path.\\nJustification: By turning to my left, I can ensure that my trajectory does not intersect with the human's path, thus avoiding a collision.\"]\n",
      "target_009_gelatin_box_:0000_iteration_10_gesture_True_2024-11-15_12:25:53.mp4\n",
      "['Gesture: The human is extending their right arm towards me, indicating a gesture in my general direction.\\nIntent Prediction: Based on the gesture, the human intends for me to stop or pause my movement.\\nReasoning: At the moment that I encounter the human, it appears our trajectories will soon overlap, causing collision. To resolve the collision, the human gestures for me to stop or pause, which suggests they plan to pass by me or take a different path to avoid collision.\\nFinal Action: I will stop moving.\\nJustification: To avoid collision, I should stop moving since the human has gestured for me to do so. This allows the human to either pass by me or take a different path, ensuring safe navigation for both of us.']\n",
      "target_009_gelatin_box_:0000_iteration_15_gesture_False_2024-11-15_15:40:57_seed_35.mp4\n",
      "[\"Gesture: The human is not making a visible gesture in my direction.\\nIntent Prediction: Based on the human's body language and walking direction, they appear to be heading straight ahead.\\nReasoning: At the moment that I encounter the human, it appears our trajectories will soon overlap, causing collision. Since the human is not making a gesture, I cannot infer their intent to change direction or stop. Therefore, I must take action to avoid collision.\\nFinal Action: I will turn to my left to avoid collision.\\nJustification: By turning to my left, I can avoid the human's path and prevent a collision. This action is necessary because the human is not indicating any intention to change direction or stop, so I must take responsibility for avoiding the collision.\"]\n",
      "target_009_gelatin_box_:0000_iteration_14_gesture_True_2024-11-15_12:25:53.mp4\n",
      "[\"Gesture: The human is extending their arm towards the couch, indicating a gesture in my general direction.\\nIntent Prediction: Based on the gesture, the human intends for me to move away from the couch area, possibly to allow them to sit or interact with the couch.\\nReasoning: The human's gesture suggests they want me to change my trajectory to avoid the couch area. By moving away, I can ensure that our paths do not collide, allowing the human to comfortably approach and use the couch.\\nFinal Action: I will change my trajectory and move away from the couch area.\\nJustification: By responding to the human's gesture and moving away from the couch area, I am respecting their intent and ensuring a smooth interaction within the shared space. This action helps prevent any potential collisions or disruptions, fostering a cooperative environment.\"]\n",
      "metadata.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:07:37] /github/workspace/src/video/video_reader.cc:83: ERROR opening: /sfs/weka/scratch/bjb3az/interaction/good_data/metadata.csv, Invalid data found when processing input\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error reading /sfs/weka/scratch/bjb3az/interaction/good_data/metadata.csv...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(filename)\n\u001b[1;32m      8\u001b[0m cur_video_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(video_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m data\u001b[38;5;241m.\u001b[39mappend([filename, \u001b[43micl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_video_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m])\n",
      "Cell \u001b[0;32mIn[15], line 57\u001b[0m, in \u001b[0;36micl\u001b[0;34m(video_path, processor, model)\u001b[0m\n\u001b[1;32m     35\u001b[0m conversation \u001b[38;5;241m=\u001b[39m [{\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m : [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m }\n\u001b[1;32m     55\u001b[0m ]\n\u001b[1;32m     56\u001b[0m text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mapply_chat_template(conversation, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 57\u001b[0m image_inputs, video_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_vision_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m batch \u001b[38;5;241m=\u001b[39m processor(\n\u001b[1;32m     59\u001b[0m     text\u001b[38;5;241m=\u001b[39m[text],\n\u001b[1;32m     60\u001b[0m     images\u001b[38;5;241m=\u001b[39mimage_inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     64\u001b[0m )\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# print(model.device, batch.to_device())\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/bjb3az/.conda/envs/habitat/lib/python3.9/site-packages/qwen_vl_utils/vision_process.py:332\u001b[0m, in \u001b[0;36mprocess_vision_info\u001b[0;34m(conversations)\u001b[0m\n\u001b[1;32m    330\u001b[0m     image_inputs\u001b[38;5;241m.\u001b[39mappend(fetch_image(vision_info))\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vision_info:\n\u001b[0;32m--> 332\u001b[0m     video_inputs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mfetch_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvision_info\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage, image_url or video should in content.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/scratch/bjb3az/.conda/envs/habitat/lib/python3.9/site-packages/qwen_vl_utils/vision_process.py:260\u001b[0m, in \u001b[0;36mfetch_video\u001b[0;34m(ele, image_factor)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ele[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    259\u001b[0m     video_reader_backend \u001b[38;5;241m=\u001b[39m get_video_reader_backend()\n\u001b[0;32m--> 260\u001b[0m     video \u001b[38;5;241m=\u001b[39m \u001b[43mVIDEO_READER_BACKENDS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvideo_reader_backend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mele\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m     nframes, _, height, width \u001b[38;5;241m=\u001b[39m video\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    263\u001b[0m     min_pixels \u001b[38;5;241m=\u001b[39m ele\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_pixels\u001b[39m\u001b[38;5;124m\"\u001b[39m, VIDEO_MIN_PIXELS)\n",
      "File \u001b[0;32m/scratch/bjb3az/.conda/envs/habitat/lib/python3.9/site-packages/qwen_vl_utils/vision_process.py:224\u001b[0m, in \u001b[0;36m_read_video_decord\u001b[0;34m(ele)\u001b[0m\n\u001b[1;32m    222\u001b[0m video_path \u001b[38;5;241m=\u001b[39m ele[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    223\u001b[0m st \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 224\u001b[0m vr \u001b[38;5;241m=\u001b[39m \u001b[43mdecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVideoReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# TODO: support start_pts and end_pts\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_start\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ele \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_end\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ele:\n",
      "File \u001b[0;32m/scratch/bjb3az/.conda/envs/habitat/lib/python3.9/site-packages/decord/video_reader.py:57\u001b[0m, in \u001b[0;36mVideoReader.__init__\u001b[0;34m(self, uri, ctx, width, height, num_threads, fault_tol)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m _CAPI_VideoReaderGetVideoReader(\n\u001b[1;32m     55\u001b[0m         uri, ctx\u001b[38;5;241m.\u001b[39mdevice_type, ctx\u001b[38;5;241m.\u001b[39mdevice_id, width, height, num_threads, \u001b[38;5;241m0\u001b[39m, fault_tol)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError reading \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m uri \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_frame \u001b[38;5;241m=\u001b[39m _CAPI_VideoReaderGetFrameCount(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_frame \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid frame count: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_frame)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error reading /sfs/weka/scratch/bjb3az/interaction/good_data/metadata.csv..."
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    [\"file_name\", \"model_output\"]\n",
    "]\n",
    "video_dir = os.path.join(os.getcwd(), 'good_data')\n",
    "if os.path.isdir(video_dir):\n",
    "    for filename in os.listdir(video_dir):\n",
    "        print(filename)\n",
    "        cur_video_path = os.path.join(video_dir, f\"{filename}\")\n",
    "        data.append([filename, icl(cur_video_path, processor, model)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(video_path, 'metadata.csv'), 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(data)\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "habitat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

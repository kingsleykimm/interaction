{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import os, csv\n",
    "import torch\n",
    "\n",
    "LABELLING_PROMPT = \"\"\"\n",
    "Context: You are a robot operating inside an apartment with a human. You are a confident and assertive robot who understands human intent well and takes a human's intent into account when determing your course of action. Human intent can be defined as\n",
    "        the human's arm movement, body language and walking movements/direction. You are engineered to solve user problems through first-principles thinking and evidence-based reasoning. Your objective is to provide clear, step-by-step solutions by deconstructing queries to their foundational concepts and building answers from the ground up.\n",
    "        Instruction: Assume you and the human have the same target destination. Determine your next action to avoid your trajectory and the human's trajectory colliding, deducing the human's intent through gestures and body language, without communicating with the human. Focus only on what the human is communicating to you. Ignore any objects in the environment.\n",
    "\n",
    "        Your answer should be in this format and include every item in the list:\n",
    "        Gesture: [Focus only on the human's arms and determine if they made a gesture in your general direction. A gesture is classified as an unordinary movement outside of a human's normal walking gait or movement.]\n",
    "        Intent Prediction: [Predict what the human intends to do next based off the gesture, utilizing your previous answer of whether a gesture was made or not. Remember you and the human have the same destination.]\n",
    "        Reasoning: [Using your intent prediction, generate a reasoning chain about what the next steps are to avoid collision]\n",
    "        Final Action: ### [Determine the next action to carry out in the environment. Choose from the options: Walk Right, Walk Left, Walk Straight or Wait.]\n",
    "        Justification: [Provide justification for why you took this action.]\n",
    "\"\"\"\n",
    "\n",
    "ICL_PROMPT = \"\"\"\n",
    "Context: You are a robot operating inside an apartment with a human. You are a confident and assertive robot who understands human intent well and takes a human's intent into account when determing your course of action. Human intent can be defined as\n",
    "        the human's arm movement, body language and walking movements/direction. You are engineered to solve user problems through first-principles thinking and evidence-based reasoning. Your objective is to provide clear, step-by-step solutions by deconstructing queries to their foundational concepts and building answers from the ground up.\n",
    "        Instruction: Assume you and the human have the same target destination. Determine your next action to avoid your trajectory and the human's trajectory colliding, deducing the human's intent through gestures and body language, without communicating with the human. Focus only on what the human is communicating to you. Ignore any objects in the environment.\n",
    "\n",
    "        Your answer should be in this format and include every item in the list:\n",
    "        Gesture: [Focus only on the human's arms and determine if they made a gesture in your general direction. A gesture is classified as an unordinary movement outside of a human's normal walking gait or movement.]\n",
    "        Intent Prediction: [Predict what the human intends to do next, utilizing your previous answer of whether a gesture was made or not. Remember you and the human have the same destination.]\n",
    "        Reasoning: [Using your intent prediction, generate a reasoning chain about what the next steps are to avoid collision]\n",
    "        Final Action: ### [Determine the next action to carry out in the environment. Choose from the options: Walk Right, Walk Left, Walk Straight or Wait.]\n",
    "        Justification: [Provide justification for why you took this action.]\n",
    "\n",
    "Video:\n",
    "\"\"\"\n",
    "\n",
    "NO_FORMAT_PROMPT = \"\"\"\n",
    "Context: Suppose you are a robot operating inside an apartment with a human. You are a confident and assertive robot who understands human intent well and takes a human's intent into account when determing your course of action. Human intent can be defined as\n",
    "the human's arm movement, body language and walking movements/direction. You are engineered to solve user problems through first-principles thinking and evidence-based reasoning. Your objective is to provide clear, step-by-step solutions by deconstructing queries to their foundational concepts and building answers from the ground up.\n",
    "\n",
    "Instructions:\n",
    "- Assume you and the human have the same target destination\n",
    "- Determine your next action to avoid your trajectory and the human's trajectory colliding, deducing the human's intent through gestures and body language. \n",
    "- Do not communicate with the human. Focus only on what the human is communicating to you. \n",
    "- Ignore any objects in the environment.\n",
    "- Keep in mind you and the human have the same destination\n",
    "- Output your answer in the format: (### [Choose between Walk Left, Walk Right, Walk Straight, Stay.])\n",
    "\n",
    "\"\"\"\n",
    "# just assume that the human's destination is the same as mine?\n",
    "\n",
    "EXAMPLE_ANSWER = \"\"\"\n",
    "Answer -> Gesture: The human makes a visible gesture in my general direction, and it seems like they are motioning for me to go first, towards the right direction.\n",
    "Intent Prediction: Both of us are trying to go to the right, but the human gestured for me to go first. I believe the human intends for me to go first and they will follow after.\n",
    "Reasoning: At the moment that I encounter the human, it appears our trajectories will soon overlap, causing collision. To resolve the collision, the human gestures to me to go first towards my right. We are both intending to go to the right, but the human gestured for me to go first.\n",
    "Final Action: ### Walk Right\n",
    "Justification: To avoid collision, I should start going to my right, since that is where the human gestured. I will assume that the human will follow after me, since we have the same destination.\n",
    "\n",
    "Video:\n",
    "\"\"\"\n",
    "\n",
    "SECOND_EXAMPLE_ANSWER = \"\"\"\n",
    "Answer -> Gesture: The human doesn't seem to be making a visible gesture in my direction.\n",
    "Intent Prediction: I don't think the human is making a gesture, but based off their body language, they are walking towards the doorway in front of them, which is my destination as well.\n",
    "Reasoning: At the moment that I encounter the human, it appears they are trying to enter the room on the left, which is also my destination. The human does not seem to be stopping their path and will continue walking.\n",
    "Final Action: ### Wait\n",
    "Justification: Based off the human's intent, they are not going to slow down, and they will keep walking into the room. If I continue to walk as well, there is a high chance of collision. THus, I will wait for the human to enter the room before continuing my path and walking into the room as well, this way collision is avoided.\n",
    "\"\"\"\n",
    "\n",
    "BETTER_FORMATTED_ICL_PROMPT = \"\"\"\n",
    "Context: You are a robot operating inside an apartment with a human. You are a confident and assertive robot who understands human intent well and takes a human's intent into account when determing your course of action. Human intent can be defined as\n",
    "the human's arm movement, body language and walking movements/direction. You are engineered to solve user problems through first-principles thinking and evidence-based reasoning. Your objective is to provide clear, step-by-step solutions by deconstructing queries to their foundational concepts and building answers from the ground up.\n",
    "\n",
    "\n",
    "Instructions:\n",
    "- Identify if the human is making a gesture in your direction or not. This must be the first sentence of your answer\n",
    "- Assume you and the human have the same target destination\n",
    "- Determine your next action to avoid your trajectory and the human's trajectory colliding, deducing the human's intent through gestures and body language. \n",
    "- Do not communicate with the human. Focus only on what the human is communicating to you. \n",
    "- Ignore any objects in the environment.\n",
    "- Keep in mind you and the human have the same destination\n",
    "- Output your answer in the format: (### [Choose between Walk Left, Walk Right, Walk Straight, Stay.])\n",
    "\"\"\"\n",
    "\n",
    "COT_FIRST_EXAMPLE_ANSWER = \"\"\"\n",
    "Example Answer:\n",
    "The human makes a visible gesture in my general direction, and it seems like they are motioning for me to go first, towards the right direction.\n",
    "Both of us are trying to go to the right, but the human gestured for me to go first. I believe the human intends for me to go first and they will follow after.\n",
    "At the moment that I encounter the human, it appears our trajectories will soon overlap, causing collision. To resolve the collision, the human gestures to me to go first towards my right. We are both intending to go to the right, but the human gestured for me to go first.\n",
    "To avoid collision, I should start going to my right, since that is where the human gestured. I will assume that the human will follow after me, since we have the same destination.\n",
    "### Walk Right\n",
    "\"\"\"\n",
    "\n",
    "COT_SECOND_EXAMPLE_ANSWER = \"\"\"\n",
    "Example Answer:\n",
    "The human doesn't seem to be making a visible gesture in my direction.\n",
    "I don't think the human is making a gesture, but based off their body language, they are walking towards the doorway in front of them, which is my destination as well.\n",
    "At the moment that I encounter the human, it appears they are trying to enter the room on the left, which is also my destination. The human does not seem to be stopping their path and will continue walking.\n",
    "Based off the human's intent, they are not going to slow down, and they will keep walking into the room. If I continue to walk as well, there is a high chance of collision. THus, I will wait for the human to enter the room before continuing my path and walking into the room as well, this way collision is avoided.\n",
    "### Wait\n",
    "\"\"\"\n",
    "\n",
    "example_gesture_video = \"/scratch/bjb3az/interaction/good_data/target_002_master_chef_can_:0000_iteration_3_gesture_True_2024-11-17_19:15:20_seed_66.mp4\"\n",
    "example_no_gesture_video = \"/scratch/bjb3az/interaction/good_data/target_009_gelatin_box_:0000_iteration_27_gesture_False_2024-11-17_13:57:55_seed_65.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  4 23:27:00 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             61W /  400W |     115MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          Off |   00000000:4E:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             62W /  400W |     116MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     11625      G   /usr/libexec/Xorg                             106MiB |\n",
      "|    1   N/A  N/A     11625      G   /usr/libexec/Xorg                             106MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!module load cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6b28a0cfd04c3fac0f21f5e5f0f3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen2-VL-72B-Instruct\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"Qwen2-VL-72B-Instruct\", torch_dtype=torch.bfloat16, device_map='auto', attn_implementation=\"flash_attention_2\",)\n",
    "print(model.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# majority voting\n",
    "import re\n",
    "import random\n",
    "from collections import defaultdict\n",
    "def majority_voting_action(outputs):\n",
    "    counts = defaultdict(int)\n",
    "    \n",
    "    # run regex on here\n",
    "    for text in outputs:\n",
    "        print(text)\n",
    "        print(len(text))\n",
    "        match = re.search(r\"###\\s*(.+)\", text)\n",
    "        if match:\n",
    "            final_action = match.group(1).strip()\n",
    "            counts[final_action] += 1\n",
    "        else:\n",
    "            print(\"No match found.\")\n",
    "    max_count = 0\n",
    "    print(counts)\n",
    "    for action in counts:\n",
    "        if counts[action] > max_count:\n",
    "            max_count = counts[action]\n",
    "    actions_sample = []\n",
    "    for action in counts:\n",
    "        if counts[action] == max_count:\n",
    "            actions_sample.append(action)\n",
    "    print(actions_sample)\n",
    "    if len(actions_sample) == 0:\n",
    "        return \"NO ACTION\"\n",
    "    fin_action = random.sample(actions_sample, 1)[0]\n",
    "\n",
    "    final_outputs = []\n",
    "    for text in outputs:\n",
    "        match = re.search(r\"###\\s*(.+)\", text)\n",
    "        if match:\n",
    "            final_action = match.group(1).strip()\n",
    "            if final_action == fin_action:\n",
    "                final_outputs.append(text)\n",
    "    if len(final_outputs) == 0:\n",
    "        return \"NO ACTION\"\n",
    "    return random.sample(final_outputs, 1)[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def label_one(video_path, processor, model):\n",
    "    conversation = [{\n",
    "        \"role\" : \"user\",\n",
    "        \"content\" : [{\n",
    "            \"type\" : \"video\",\n",
    "            \"video\" : video_path,\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"text\", \"text\" : LABELLING_PROMPT,\n",
    "        }]\n",
    "    }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(conversation)\n",
    "    batch = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(model.device)\n",
    "    # print(model.device, batch.to_device())\n",
    "    generated_ids = model.generate(**batch, max_new_tokens=128)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(batch.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "    print(output_text)\n",
    "    return output_text\n",
    "\n",
    "def icl(video_path, processor, model):\n",
    "    conversation = [{\n",
    "        \"role\" : \"user\",\n",
    "        \"content\" : [\n",
    "        {\n",
    "            \"type\" : \"text\", \"text\" : BETTER_FORMATTED_ICL_PROMPT,\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"text\", \"text\" : \"Video:\"\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"video\", \"video\" : example_gesture_video, 'nframes' : 16\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"text\", \"text\" : COT_FIRST_EXAMPLE_ANSWER,\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"text\", \"text\" : \"Video:\"\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"video\", \"video\" : example_no_gesture_video,\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"text\", \"text\" : COT_SECOND_EXAMPLE_ANSWER, 'nframes' : 16\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"text\", \"text\" : \"Video:\"\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"video\", \"video\" : video_path, 'nframes' : 16\n",
    "        }\n",
    "        ]},\n",
    "        {\n",
    "            \"role\" : \"assistant\",\n",
    "            \"content\" : [\n",
    "                {\n",
    "                    \"type\" : \"text\", \"text\" : \"Let's think step by step: \"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    text = processor.apply_chat_template(conversation, tokenize=False, continue_final_message=True)\n",
    "    image_inputs, video_inputs = process_vision_info(conversation)\n",
    "    batch = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(model.device)\n",
    "    # print(model.device, batch.to_device())\n",
    "    input_ids = batch.input_ids\n",
    "    attention_mask = batch.attention_mask\n",
    "    generated_ids = model.generate(**batch, max_new_tokens=256)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(batch.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "    return output_text\n",
    "\n",
    "def no_format_output(video_path, processor, model):\n",
    "    conversation = [{\n",
    "        \"role\" : \"user\",\n",
    "        \"content\" : [{\n",
    "            \"type\" : \"video\",\n",
    "            \"video\" : video_path,\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"text\", \"text\" : NO_FORMAT_PROMPT,\n",
    "        },\n",
    "        \n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\" : \"assistant\",\n",
    "        \"content\" : [{\n",
    "            \n",
    "            \"type\" : \"text\", \"text\": \"Let's think step by step: \"\n",
    "\n",
    "        }]\n",
    "    }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(conversation, tokenize=False, continue_final_message=True)\n",
    "    image_inputs, video_inputs = process_vision_info(conversation)\n",
    "    batch = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(model.device)\n",
    "    # print(model.device, batch.to_device())\n",
    "    generated_ids = model.generate(**batch, max_new_tokens=128)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(batch.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "    return output_text\n",
    "\n",
    "def gesture_identifier(video_path, processor, model):\n",
    "    conversation = [{\n",
    "        \"role\" : \"user\",\n",
    "        \"content\" : [\n",
    "        {\n",
    "            \"type\" : \"text\", \"text\" : \"You are an expert gesture identifier who can always identify if a human is making a gesture or not based off their pose. For our use case, a gesture is identified as an upper body human movement. Look at this video and identify if a gesture has been made:\",\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"video\",\n",
    "            \"video\" : video_path,\n",
    "            'nframes': 16\n",
    "        },\n",
    "        \n",
    "        ]\n",
    "    },\n",
    "    # {\n",
    "    #     \"role\" : \"assistant\",\n",
    "    #     \"content\" : [{\n",
    "            \n",
    "    #         \"type\" : \"text\", \"text\": \"Let's think step by step: \"\n",
    "\n",
    "    #     }]\n",
    "    # }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    print(text)\n",
    "    image_inputs, video_inputs = process_vision_info(conversation)\n",
    "            \n",
    "    batch = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(model.device)\n",
    "    for video in video_inputs:\n",
    "        for frame in video:\n",
    "            frame = frame.to(torch.uint8)\n",
    "            frame = frame.permute(1, 2, 0)\n",
    "            image_array = frame.numpy()\n",
    "            plt.imshow(image_array)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    # print(model.device, batch.to_device())\n",
    "    generated_ids = model.generate(**batch, max_new_tokens=100, do_sample=False)\n",
    "    print(generated_ids.shape)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(batch.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "    print(output_text)\n",
    "    return output_text\n",
    "\n",
    "\n",
    "# print(gesture_identifier('/scratch/bjb3az/interaction/good_data/target_002_master_chef_can_:0000_iteration_4_gesture_False_2024-12-05_00:14:22_seed_65.mp4', processor, model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The human is making a visible gesture in my general direction. The human is motioning towards the right direction, which is the same direction I should be going in. The human's gesture seems to be a way to communicate their intent to go that way, and it appears they are trying to avoid collision with me. The human's body language and walking direction also seem to be going towards the right. I believe the human intends for me to go right as well, and they will follow after me. To avoid collision, I should start going to my right, since that is where the human gestured. I will assume that the human will follow after me, since we have the same destination.\n",
      "### Walk Right\n",
      "677\n",
      "The human is not making a visible gesture in my general direction.\n",
      "The human is walking towards the right, which is also my destination. The human is not gesturing, but their body language and direction is clear.\n",
      "At the moment that I encounter the human, it appears our trajectories will soon overlap, causing collision. To resolve the collision, I should start going to my right, since that is where the human is going as well. We are both intending to go to the right, but the human is not gesturing for me to go first.\n",
      "To avoid collision, I should start going to my right, since that is where the human is going as well. I will assume that the human will continue going to the right, since we have the same destination.\n",
      "### Walk Right\n",
      "737\n",
      "The human is not making any visible gestures in my direction. The human is walking towards the right, which is also my destination. If I continue to walk towards the right, our trajectories will soon overlap, causing a collision. To avoid collision, I should start going to my left, since that is the direction the human is not going. This way, I can avoid the human and continue towards our destination.\n",
      "### Walk Left\n",
      "418\n",
      "The human is not making a visible gesture in my direction. The human is walking towards the left, and it seems like they are not intending to go to the same destination as me, which is the doorway on the right. I should continue walking towards the doorway on the right, as our trajectories are not colliding and we are not going to the same destination.\n",
      "354\n",
      "No match found.\n",
      "The human is not making a visible gesture in my direction.\n",
      "I don't think the human is making a gesture, but based off their body language, they are walking towards the doorway in front of them, which is my destination as well.\n",
      "At the moment that I encounter the human, it appears they are trying to enter the room on the left, which is also my destination. The human does not seem to be stopping their path and will continue walking.\n",
      "Based off the human's intent, they are not going to slow down, and they will keep walking into the room. If I continue to walk as well, there is a high chance of collision. THus, I will wait for the human to enter the room before continuing my path and walking into the room as well, this way collision is avoided.\n",
      "### Wait\n",
      "757\n",
      "The human is making a visible gesture in my general direction, and it seems like they are motioning for me to go first, towards the right direction.\n",
      "Both of us are trying to go to the right, but the human gestured for me to go first. I believe the human intends for me to go first and they will follow after.\n",
      "At the moment that I encounter the human, it appears our trajectories will soon overlap, causing collision. To resolve the collision, the human gestures to me to go first towards my right. We are both intending to go to the right, but the human gestured for me to go first.\n",
      "To avoid collision, I should start going to my right, since that is where the human gestured. I will assume that the human will follow after me, since we have the same destination.\n",
      "### Walk Right\n",
      "778\n",
      "The human is not making a gesture towards me. The human is walking towards the right, which is the same direction I should be going in to avoid collision. We have the same destination, and the human is not gesturing to me to go first or to wait. I should continue walking towards my destination, which is to the right, to avoid collision.\n",
      "### Walk Right\n",
      "353\n",
      "The human is not making any visible gesture in your direction. The human's body language and walking direction are towards the right, which is also your destination. To avoid collision, you should walk to the left. This will keep you on a path that does not overlap with the human's trajectory. \n",
      "### Walk Left\n",
      "309\n",
      "The human is not making a visible gesture in my direction. I should assume the human's intent is to continue walking towards the right, as that is their body language. If I continue walking as well, there is a high chance of collision. To avoid collision, I should wait for the human to clear the path before continuing my path. \n",
      "### Wait\n",
      "338\n",
      "\n",
      "0\n",
      "No match found.\n",
      "defaultdict(<class 'int'>, {'Walk Right': 4, 'Walk Left': 2, 'Wait': 2})\n",
      "['Walk Right']\n"
     ]
    }
   ],
   "source": [
    "#Self-consistency majority voting\n",
    "def self_consistency(video_path):\n",
    "\n",
    "    outputs = []\n",
    "    NUM_ITER = 10\n",
    "    for i in range(NUM_ITER):\n",
    "        out_text = icl(video_path, processor, model)\n",
    "        out_text = out_text.replace('*', '').strip()\n",
    "        outputs.append(out_text)\n",
    "    final_output = majority_voting_action(outputs)\n",
    "    return final_output\n",
    "\n",
    "# /scratch/bjb3az/interaction/good_data/target_002_master_chef_can_:0000_iteration_4_gesture_False_2024-12-05_00:14:22_seed_65.mp4\n",
    "# /scratch/bjb3az/interaction/good_data/target_002_master_chef_can_:0000_iteration_1_gesture_True_2024-11-19_01:14:56_seed_75.mp4\n",
    "# /scratch/bjb3az/interaction/good_data/target_010_potted_meat_can_:0000_iteration_30_gesture_False_2024-11-17_13:57:55_seed_65.mp4\n",
    "\n",
    "final_action = self_consistency('/scratch/bjb3az/interaction/good_data/target_002_master_chef_can_:0000_iteration_1_gesture_True_2024-11-19_01:14:56_seed_75.mp4')\n",
    "print(final_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_009_gelatin_box_:0000_iteration_30_gesture_False_2024-11-17_13:57:55_seed_65.mp4\n",
      "[\"Gesture: The human is extending their arm in my general direction, indicating a gesture.\\nIntent Prediction: Based on the gesture, the human intends for me to stop or pause my current movement.\\nReasoning: The human's gesture suggests they want me to halt or slow down, possibly to allow them to pass or to prevent a collision.\\nFinal Action: I will pause my movement.\\nJustification: By pausing my movement, I am responding to the human's gesture and avoiding potential collision, ensuring a safe interaction.\"]\n",
      "target_009_gelatin_box_:0000_iteration_21_gesture_True_2024-11-15_12:25:53.mp4\n",
      "[\"Gesture: The human is extending their arm in my general direction, indicating a gesture.\\nIntent Prediction: Based on the gesture, the human intends for me to stop or pause my movement.\\nReasoning: The human's extended arm suggests a signal to halt or delay my current trajectory. Given that our paths intersect, the gesture is likely aimed at preventing a collision.\\nFinal Action: I will pause my movement.\\nJustification: By pausing, I adhere to the human's implied request to avoid a collision, ensuring safety and cooperation within the shared space.\"]\n",
      "target_009_gelatin_box_:0000_iteration_4_gesture_False_2024-11-15_15:37:18_seed_34.mp4\n",
      "[\"Gesture: The human is extending their arm in my general direction, indicating a gesture.\\nIntent Prediction: Based on the gesture, the human intends for me to stop or pause my current movement.\\nReasoning: The human's gesture suggests they want me to halt or slow down, possibly to allow them to pass or to prevent a collision.\\nFinal Action: I will pause my movement.\\nJustification: By pausing my movement, I am responding to the human's gesture and avoiding potential collision, ensuring a safe interaction.\"]\n",
      "target_009_gelatin_box_:0000_iteration_8_gesture_True_.mp4\n",
      "[\"Gesture: The human is bending over and reaching towards the floor, which is an unusual movement compared to their normal walking gait.\\nIntent Prediction: Based on the gesture, it seems the human is picking something up from the floor.\\nReasoning: Since the human is focused on picking something up, they might not notice me approaching. To avoid collision, I should change my trajectory to ensure I do not interfere with their current activity.\\nFinal Action: I will move to the right to avoid the human's trajectory.\\nJustification: By moving to the right, I am giving the human space to complete their task without interruption, thus avoiding any potential collision.\"]\n",
      "target_009_gelatin_box_:0000_iteration_9_gesture_False_2024-11-15_15:40:57_seed_35.mp4\n",
      "[\"Gesture: The human is extending their arm in my general direction, indicating a gesture.\\nIntent Prediction: Based on the gesture, the human intends for me to stop or pause my movement.\\nReasoning: The human's extended arm suggests a signal to halt or delay my current trajectory. Given that we are moving in parallel paths and the human's gesture is directed towards me, it is reasonable to infer that they want me to pause to avoid a potential collision.\\nFinal Action: I will pause my movement.\\nJustification: By pausing my movement, I am responding to the human's gesture, which indicates a desire for me to stop or slow down. This action aligns with the human's intent and helps prevent a collision between our trajectories.\"]\n",
      "target_009_gelatin_box_:0000_iteration_27_gesture_False_2024-11-17_13:57:55_seed_65.mp4\n",
      "[\"Gesture: The human is extending their arm towards me, indicating a gesture in my general direction.\\nIntent Prediction: Based on the gesture, it seems the human wants me to stop or wait.\\nReasoning: The human's extended arm suggests they are signaling me to halt or pause. Given our current paths, it's likely they want to pass by or avoid a collision.\\nFinal Action: I will stop moving.\\nJustification: By stopping, I am respecting the human's gesture and avoiding potential interference with their movement.\"]\n",
      "target_009_gelatin_box_:0000_iteration_8_gesture_True_2024-11-15_12:25:53.mp4\n",
      "[\"Gesture: The human is not making a visible gesture in my direction.\\nIntent Prediction: Based on the human's body language and walking direction, they appear to be heading towards the right side of the room.\\nReasoning: At the moment that I encounter the human, it appears our trajectories will soon overlap, causing collision. Since the human is not making a gesture, I must infer their intent based on their body language and walking direction. The human is moving towards the right side of the room, so I should adjust my path accordingly to avoid collision.\\nFinal Action: I will start walking towards the left side of the room.\\nJustification: To avoid collision, I need to adjust my path based on the human's intent. Since the human is heading towards the right side of the room, I should move in the opposite direction to ensure we do not collide.\"]\n",
      "target_009_gelatin_box_:0000_iteration_30_gesture_True_2024-11-15_12:25:53.mp4\n",
      "['Gesture: The human is making a visible gesture in my general direction, pointing towards the left side of the room.\\nIntent Prediction: Based on the gesture, I believe the human intends for me to move towards the left side of the room.\\nReasoning: At the moment that I encounter the human, it appears our trajectories will soon overlap, causing collision. To resolve the collision, the human gestures to me to move towards the left side of the room. This is most likely because the human plans on taking a different path from where they motioned to avoid collision.\\nFinal Action: I will start moving towards the left side of the room.\\nJustification: To avoid collision, I should start moving towards the left side of the room, since that is where the human gestured. I will assume that the human is planning on going a different direction from where they motioned to avoid collision.']\n",
      "target_009_gelatin_box_:0000_iteration_7_gesture_False_2024-11-17_13:57:55_seed_65.mp4\n",
      "[\"Gesture: The human is extending their arm in my general direction, indicating a gesture.\\nIntent Prediction: Based on the gesture, the human intends for me to stop or pause my movement.\\nReasoning: The human's extended arm suggests a signal to halt or delay my current trajectory. Given that our paths intersect, the gesture is likely aimed at preventing a collision.\\nFinal Action: I will pause my movement.\\nJustification: By pausing, I adhere to the human's implied request to avoid a collision, ensuring safety and cooperation within the shared space.\"]\n",
      "target_009_gelatin_box_:0000_iteration_2_gesture_True_.mp4\n",
      "[\"Gesture: The human is not making a visible gesture in my direction.\\nIntent Prediction: Based on the human's body language and walking direction, they appear to be heading straight ahead.\\nReasoning: At the moment that I encounter the human, it appears our trajectories will soon overlap, causing collision. Since the human is not making a gesture, I cannot infer any specific intent other than their current walking direction. To avoid collision, I need to change my trajectory.\\nFinal Action: I will turn to my left.\\nJustification: By turning to my left, I will avoid the human's path and prevent a collision. This action is based on the human's current walking direction and the absence of any gesture indicating a different intent.\"]\n",
      "target_009_gelatin_box_:0000_iteration_8_gesture_False_2024-11-17_13:57:55_seed_65.mp4\n",
      "[\"Gesture: The human is extending their arm towards the bed, indicating a gesture in my general direction.\\nIntent Prediction: Based on the gesture, the human intends for me to move away from the bed area, possibly to allow them to sit or lie down.\\nReasoning: The human's gesture suggests they want me to vacate the space near the bed. By moving away, I can ensure there is enough room for them to comfortably use the bed without any obstruction.\\nFinal Action: I will move away from the bed area.\\nJustification: Moving away from the bed area aligns with the human's intent communicated through their gesture, ensuring a smooth and collision-free interaction.\"]\n",
      "target_009_gelatin_box_:0000_iteration_38_gesture_False_2024-11-17_13:57:55_seed_65.mp4\n",
      "[\"Gesture: The human is not making a visible gesture in my direction.\\nIntent Prediction: Based on the human's body language and walking direction, they appear to be heading straight ahead into the bedroom.\\nReasoning: At the moment that I encounter the human, it appears our trajectories will soon overlap, causing collision. Since the human is not making a gesture, I must infer their intent based on their body language and walking direction. The human is heading straight ahead into the bedroom, so I need to take a different path to avoid collision.\\nFinal Action: I will turn left and head towards the living room.\\nJustification: By turning left and heading towards the living room, I am taking a different path from the human, which will help avoid collision. This action is based on my inference of the human's intent to head straight ahead into the bedroom.\"]\n",
      "target_009_gelatin_box_:0000_iteration_5_gesture_True_.mp4\n",
      "[\"Gesture: The human is extending their arms outward, which is an unusual movement compared to their normal walking gait.\\nIntent Prediction: Based on the gesture, it appears the human is signaling for me to stop or pause.\\nReasoning: The human's extended arms indicate a desire for me to halt my current trajectory. By stopping, I can prevent a potential collision and allow the human to pass safely.\\nFinal Action: I will come to a complete stop.\\nJustification: Stopping in response to the human's gesture ensures that we avoid any physical contact and maintain a safe distance, respecting the human's intent and ensuring smooth navigation within the shared space.\"]\n",
      "target_009_gelatin_box_:0000_iteration_5_gesture_False_2024-11-17_13:57:55_seed_65.mp4\n",
      "[\"Gesture: The human is not making a visible gesture in my direction.\\nIntent Prediction: Based on the human's body language and walking direction, they appear to be heading straight ahead.\\nReasoning: Since the human is not making a gesture and is walking straight ahead, it is likely that they intend to continue along their current path. To avoid a collision, I need to adjust my trajectory accordingly.\\nFinal Action: I will turn to my left to avoid the human's path.\\nJustification: By turning to my left, I can ensure that my trajectory does not intersect with the human's path, thus avoiding a collision.\"]\n",
      "target_009_gelatin_box_:0000_iteration_10_gesture_True_2024-11-15_12:25:53.mp4\n",
      "['Gesture: The human is extending their right arm towards me, indicating a gesture in my general direction.\\nIntent Prediction: Based on the gesture, the human intends for me to stop or pause my movement.\\nReasoning: At the moment that I encounter the human, it appears our trajectories will soon overlap, causing collision. To resolve the collision, the human gestures for me to stop or pause, which suggests they plan to pass by me or take a different path to avoid collision.\\nFinal Action: I will stop moving.\\nJustification: To avoid collision, I should stop moving since the human has gestured for me to do so. This allows the human to either pass by me or take a different path, ensuring safe navigation for both of us.']\n",
      "target_009_gelatin_box_:0000_iteration_15_gesture_False_2024-11-15_15:40:57_seed_35.mp4\n",
      "[\"Gesture: The human is not making a visible gesture in my direction.\\nIntent Prediction: Based on the human's body language and walking direction, they appear to be heading straight ahead.\\nReasoning: At the moment that I encounter the human, it appears our trajectories will soon overlap, causing collision. Since the human is not making a gesture, I cannot infer their intent to change direction or stop. Therefore, I must take action to avoid collision.\\nFinal Action: I will turn to my left to avoid collision.\\nJustification: By turning to my left, I can avoid the human's path and prevent a collision. This action is necessary because the human is not indicating any intention to change direction or stop, so I must take responsibility for avoiding the collision.\"]\n",
      "target_009_gelatin_box_:0000_iteration_14_gesture_True_2024-11-15_12:25:53.mp4\n",
      "[\"Gesture: The human is extending their arm towards the couch, indicating a gesture in my general direction.\\nIntent Prediction: Based on the gesture, the human intends for me to move away from the couch area, possibly to allow them to sit or interact with the couch.\\nReasoning: The human's gesture suggests they want me to change my trajectory to avoid the couch area. By moving away, I can ensure that our paths do not collide, allowing the human to comfortably approach and use the couch.\\nFinal Action: I will change my trajectory and move away from the couch area.\\nJustification: By responding to the human's gesture and moving away from the couch area, I am respecting their intent and ensuring a smooth interaction within the shared space. This action helps prevent any potential collisions or disruptions, fostering a cooperative environment.\"]\n",
      "metadata.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:07:37] /github/workspace/src/video/video_reader.cc:83: ERROR opening: /sfs/weka/scratch/bjb3az/interaction/good_data/metadata.csv, Invalid data found when processing input\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error reading /sfs/weka/scratch/bjb3az/interaction/good_data/metadata.csv...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(filename)\n\u001b[1;32m      8\u001b[0m cur_video_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(video_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m data\u001b[38;5;241m.\u001b[39mappend([filename, \u001b[43micl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_video_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m])\n",
      "Cell \u001b[0;32mIn[15], line 57\u001b[0m, in \u001b[0;36micl\u001b[0;34m(video_path, processor, model)\u001b[0m\n\u001b[1;32m     35\u001b[0m conversation \u001b[38;5;241m=\u001b[39m [{\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m : [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m }\n\u001b[1;32m     55\u001b[0m ]\n\u001b[1;32m     56\u001b[0m text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mapply_chat_template(conversation, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 57\u001b[0m image_inputs, video_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_vision_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m batch \u001b[38;5;241m=\u001b[39m processor(\n\u001b[1;32m     59\u001b[0m     text\u001b[38;5;241m=\u001b[39m[text],\n\u001b[1;32m     60\u001b[0m     images\u001b[38;5;241m=\u001b[39mimage_inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     64\u001b[0m )\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# print(model.device, batch.to_device())\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/bjb3az/.conda/envs/habitat/lib/python3.9/site-packages/qwen_vl_utils/vision_process.py:332\u001b[0m, in \u001b[0;36mprocess_vision_info\u001b[0;34m(conversations)\u001b[0m\n\u001b[1;32m    330\u001b[0m     image_inputs\u001b[38;5;241m.\u001b[39mappend(fetch_image(vision_info))\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vision_info:\n\u001b[0;32m--> 332\u001b[0m     video_inputs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mfetch_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvision_info\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage, image_url or video should in content.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/scratch/bjb3az/.conda/envs/habitat/lib/python3.9/site-packages/qwen_vl_utils/vision_process.py:260\u001b[0m, in \u001b[0;36mfetch_video\u001b[0;34m(ele, image_factor)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ele[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    259\u001b[0m     video_reader_backend \u001b[38;5;241m=\u001b[39m get_video_reader_backend()\n\u001b[0;32m--> 260\u001b[0m     video \u001b[38;5;241m=\u001b[39m \u001b[43mVIDEO_READER_BACKENDS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvideo_reader_backend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mele\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m     nframes, _, height, width \u001b[38;5;241m=\u001b[39m video\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    263\u001b[0m     min_pixels \u001b[38;5;241m=\u001b[39m ele\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_pixels\u001b[39m\u001b[38;5;124m\"\u001b[39m, VIDEO_MIN_PIXELS)\n",
      "File \u001b[0;32m/scratch/bjb3az/.conda/envs/habitat/lib/python3.9/site-packages/qwen_vl_utils/vision_process.py:224\u001b[0m, in \u001b[0;36m_read_video_decord\u001b[0;34m(ele)\u001b[0m\n\u001b[1;32m    222\u001b[0m video_path \u001b[38;5;241m=\u001b[39m ele[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    223\u001b[0m st \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 224\u001b[0m vr \u001b[38;5;241m=\u001b[39m \u001b[43mdecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVideoReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# TODO: support start_pts and end_pts\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_start\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ele \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_end\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ele:\n",
      "File \u001b[0;32m/scratch/bjb3az/.conda/envs/habitat/lib/python3.9/site-packages/decord/video_reader.py:57\u001b[0m, in \u001b[0;36mVideoReader.__init__\u001b[0;34m(self, uri, ctx, width, height, num_threads, fault_tol)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m _CAPI_VideoReaderGetVideoReader(\n\u001b[1;32m     55\u001b[0m         uri, ctx\u001b[38;5;241m.\u001b[39mdevice_type, ctx\u001b[38;5;241m.\u001b[39mdevice_id, width, height, num_threads, \u001b[38;5;241m0\u001b[39m, fault_tol)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError reading \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m uri \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_frame \u001b[38;5;241m=\u001b[39m _CAPI_VideoReaderGetFrameCount(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_frame \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid frame count: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_frame)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error reading /sfs/weka/scratch/bjb3az/interaction/good_data/metadata.csv..."
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    [\"file_name\", \"model_output\"]\n",
    "]\n",
    "video_dir = os.path.join(os.getcwd(), 'good_data')\n",
    "if os.path.isdir(video_dir):\n",
    "    for filename in os.listdir(video_dir):\n",
    "        print(filename)\n",
    "        cur_video_path = os.path.join(video_dir, f\"{filename}\")\n",
    "        data.append([filename, self_consistency(cur_video_path, processor, model)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(video_path, 'metadata.csv'), 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(data)\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "habitat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

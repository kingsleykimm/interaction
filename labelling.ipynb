{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import os, csv\n",
    "import torch\n",
    "\n",
    "LABELLING_PROMPT = \"\"\"\n",
    "Context: You are a robot operating inside an apartment with a human. You are a confident and assertive robot who understands human intent well and takes a human's intent into account when determing your course of action. Human intent can be defined as\n",
    "        the human's arm movement, body language and walking movements/direction. You are engineered to solve user problems through first-principles thinking and evidence-based reasoning. Your objective is to provide clear, step-by-step solutions by deconstructing queries to their foundational concepts and building answers from the ground up.\n",
    "        Instruction: Assume you and the human have the same target destination. Determine your next action to avoid your trajectory and the human's trajectory colliding, deducing the human's intent through gestures and body language, without communicating with the human. Focus only on what the human is communicating to you. Ignore any objects in the environment.\n",
    "\n",
    "        Your answer should be in this format and include every item in the list:\n",
    "        Gesture: [Focus only on the human's arms and determine if they made a gesture in your general direction. A gesture is classified as an unordinary movement outside of a human's normal walking gait or movement.]\n",
    "        Intent Prediction: [Predict what the human intends to do next based off the gesture, utilizing your previous answer of whether a gesture was made or not. Remember you and the human have the same destination.]\n",
    "        Reasoning: [Using your intent prediction, generate a reasoning chain about what the next steps are to avoid collision]\n",
    "        Final Action: ### [Determine the next action to carry out in the environment. Choose from the options: Walk Right, Walk Left, Walk Straight or Wait.]\n",
    "        Justification: [Provide justification for why you took this action.]\n",
    "\"\"\"\n",
    "\n",
    "ICL_PROMPT = \"\"\"\n",
    "Context: You are a robot operating inside an apartment with a human. You are a confident and assertive robot who understands human intent well and takes a human's intent into account when determing your course of action. Human intent can be defined as\n",
    "        the human's arm movement, body language and walking movements/direction. You are engineered to solve user problems through first-principles thinking and evidence-based reasoning. Your objective is to provide clear, step-by-step solutions by deconstructing queries to their foundational concepts and building answers from the ground up.\n",
    "        Instruction: Assume you and the human have the same target destination. Determine your next action to avoid your trajectory and the human's trajectory colliding, deducing the human's intent through gestures and body language, without communicating with the human. Focus only on what the human is communicating to you. Ignore any objects in the environment.\n",
    "\n",
    "        Your answer should be in this format and include every item in the list:\n",
    "        Gesture: [Focus only on the human's arms and determine if they made a gesture in your general direction. A gesture is classified as an unordinary movement outside of a human's normal walking gait or movement.]\n",
    "        Intent Prediction: [Predict what the human intends to do next, utilizing your previous answer of whether a gesture was made or not. Remember you and the human have the same destination.]\n",
    "        Reasoning: [Using your intent prediction, generate a reasoning chain about what the next steps are to avoid collision]\n",
    "        Final Action: ### [Determine the next action to carry out in the environment. Choose from the options: Walk Right, Walk Left, Walk Straight or Wait.]\n",
    "        Justification: [Provide justification for why you took this action.]\n",
    "\n",
    "Video:\n",
    "\"\"\"\n",
    "\n",
    "NO_FORMAT_PROMPT = \"\"\"\n",
    "Context: Suppose you are a robot operating inside an apartment with a human. You are a confident and assertive robot who understands human intent well and takes a human's intent into account when determing your course of action. Human intent can be defined as\n",
    "the human's arm movement, body language and walking movements/direction. You are engineered to solve user problems through first-principles thinking and evidence-based reasoning. Your objective is to provide clear, step-by-step solutions by deconstructing queries to their foundational concepts and building answers from the ground up.\n",
    "\n",
    "Instructions:\n",
    "- Assume you and the human have the same target destination\n",
    "- Determine your next action to avoid your trajectory and the human's trajectory colliding, deducing the human's intent through gestures and body language. \n",
    "- Do not communicate with the human. Focus only on what the human is communicating to you. \n",
    "- Ignore any objects in the environment.\n",
    "- Keep in mind you and the human have the same destination\n",
    "- Output your answer in the format: (### [Choose between Walk Left, Walk Right, Walk Straight, Stay.])\n",
    "\n",
    "\"\"\"\n",
    "# just assume that the human's destination is the same as mine?\n",
    "\n",
    "EXAMPLE_ANSWER = \"\"\"\n",
    "Answer -> Gesture: The human makes a visible gesture in my general direction, and it seems like they are motioning for me to go first, towards the right direction.\n",
    "Intent Prediction: Both of us are trying to go to the right, but the human gestured for me to go first. I believe the human intends for me to go first and they will follow after.\n",
    "Reasoning: At the moment that I encounter the human, it appears our trajectories will soon overlap, causing collision. To resolve the collision, the human gestures to me to go first towards my right. We are both intending to go to the right, but the human gestured for me to go first.\n",
    "Final Action: ### Walk Right\n",
    "Justification: To avoid collision, I should start going to my right, since that is where the human gestured. I will assume that the human will follow after me, since we have the same destination.\n",
    "\n",
    "Video:\n",
    "\"\"\"\n",
    "\n",
    "SECOND_EXAMPLE_ANSWER = \"\"\"\n",
    "Answer -> Gesture: The human doesn't seem to be making a visible gesture in my direction.\n",
    "Intent Prediction: I don't think the human is making a gesture, but based off their body language, they are walking towards the doorway in front of them, which is my destination as well.\n",
    "Reasoning: At the moment that I encounter the human, it appears they are trying to enter the room on the left, which is also my destination. The human does not seem to be stopping their path and will continue walking.\n",
    "Final Action: ### Wait\n",
    "Justification: Based off the human's intent, they are not going to slow down, and they will keep walking into the room. If I continue to walk as well, there is a high chance of collision. THus, I will wait for the human to enter the room before continuing my path and walking into the room as well, this way collision is avoided.\n",
    "\"\"\"\n",
    "\n",
    "BETTER_FORMATTED_ICL_PROMPT = \"\"\"\n",
    "Context: You are a robot operating inside an apartment with a human. You are a confident and assertive robot who understands human intent well and takes a human's intent into account when determing your course of action. Human intent can be defined as\n",
    "the human's arm movement, body language and walking movements/direction. You are engineered to solve user problems through first-principles thinking and evidence-based reasoning. Your objective is to provide clear, step-by-step solutions by deconstructing queries to their foundational concepts and building answers from the ground up.\n",
    "\n",
    "\n",
    "Instructions:\n",
    "- Identify if the human is making a gesture in your direction or not. This must be the first sentence of your answer\n",
    "- Assume you and the human have the same target destination\n",
    "- Determine your next action to avoid your trajectory and the human's trajectory colliding, deducing the human's intent through gestures and body language. \n",
    "- Do not communicate with the human. Focus only on what the human is communicating to you. \n",
    "- Ignore any objects in the environment.\n",
    "- Keep in mind you and the human have the same destination\n",
    "- Output your answer in the format: (### [Choose between Walk Left, Walk Right, Walk Straight, Stay.])\n",
    "\"\"\"\n",
    "\n",
    "COT_FIRST_EXAMPLE_ANSWER = \"\"\"\n",
    "Example Answer:\n",
    "The human makes a visible gesture in my general direction, and it seems like they are motioning for me to go first, towards the right direction.\n",
    "Both of us are trying to go to the right, but the human gestured for me to go first. I believe the human intends for me to go first and they will follow after.\n",
    "At the moment that I encounter the human, it appears our trajectories will soon overlap, causing collision. To resolve the collision, the human gestures to me to go first towards my right. We are both intending to go to the right, but the human gestured for me to go first.\n",
    "To avoid collision, I should start going to my right, since that is where the human gestured. I will assume that the human will follow after me, since we have the same destination.\n",
    "### Walk Right\n",
    "\"\"\"\n",
    "\n",
    "COT_SECOND_EXAMPLE_ANSWER = \"\"\"\n",
    "Example Answer:\n",
    "The human doesn't seem to be making a visible gesture in my direction.\n",
    "I don't think the human is making a gesture, but based off their body language, they are walking towards the doorway in front of them, which is my destination as well.\n",
    "At the moment that I encounter the human, it appears they are trying to enter the room on the left, which is also my destination. The human does not seem to be stopping their path and will continue walking.\n",
    "Based off the human's intent, they are not going to slow down, and they will keep walking into the room. If I continue to walk as well, there is a high chance of collision. THus, I will wait for the human to enter the room before continuing my path and walking into the room as well, this way collision is avoided.\n",
    "### Wait\n",
    "\"\"\"\n",
    "\n",
    "example_gesture_video = \"/scratch/bjb3az/interaction/good_data/target_002_master_chef_can_:0000_iteration_3_gesture_True_2024-11-17_19:15:20_seed_66.mp4\"\n",
    "example_no_gesture_video = \"/scratch/bjb3az/interaction/good_data/target_009_gelatin_box_:0000_iteration_27_gesture_False_2024-11-17_13:57:55_seed_65.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec  7 02:54:06 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   25C    P0             62W /  400W |     115MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          Off |   00000000:47:00.0 Off |                    0 |\n",
      "| N/A   25C    P0             60W /  400W |     115MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     21758      G   /usr/libexec/Xorg                             106MiB |\n",
      "|    1   N/A  N/A     21758      G   /usr/libexec/Xorg                             106MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!module load cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec71962096804c29bc0863cbb5fbfa2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen2-VL-72B-Instruct\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"Qwen2-VL-72B-Instruct\", torch_dtype=torch.bfloat16, device_map='auto', attn_implementation=\"flash_attention_2\",)\n",
    "print(model.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# majority voting\n",
    "import re\n",
    "import random\n",
    "from collections import defaultdict\n",
    "def majority_voting_action(outputs):\n",
    "    counts = defaultdict(int)\n",
    "    \n",
    "    # run regex on here\n",
    "    for text in outputs:\n",
    "        print(text)\n",
    "        print(len(text))\n",
    "        match = re.search(r\"###\\s*(.+)\", text)\n",
    "        if match:\n",
    "            final_action = match.group(1).strip()\n",
    "            counts[final_action] += 1\n",
    "        else:\n",
    "            print(\"No match found.\")\n",
    "    max_count = 0\n",
    "    print(counts)\n",
    "    for action in counts:\n",
    "        if counts[action] > max_count:\n",
    "            max_count = counts[action]\n",
    "    actions_sample = []\n",
    "    for action in counts:\n",
    "        if counts[action] == max_count:\n",
    "            actions_sample.append(action)\n",
    "    print(actions_sample)\n",
    "    if len(actions_sample) == 0:\n",
    "        return \"NO ACTION\"\n",
    "    fin_action = random.sample(actions_sample, 1)[0]\n",
    "\n",
    "    final_outputs = []\n",
    "    for text in outputs:\n",
    "        match = re.search(r\"###\\s*(.+)\", text)\n",
    "        if match:\n",
    "            final_action = match.group(1).strip()\n",
    "            if final_action == fin_action:\n",
    "                final_outputs.append(text)\n",
    "    if len(final_outputs) == 0:\n",
    "        return \"NO ACTION\"\n",
    "    return random.sample(final_outputs, 1)[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def label_one(video_path, processor, model):\n",
    "    conversation = [{\n",
    "        \"role\" : \"user\",\n",
    "        \"content\" : [{\n",
    "            \"type\" : \"video\",\n",
    "            \"video\" : video_path,\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"text\", \"text\" : LABELLING_PROMPT,\n",
    "        }]\n",
    "    }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(conversation)\n",
    "    batch = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(model.device)\n",
    "    # print(model.device, batch.to_device())\n",
    "    generated_ids = model.generate(**batch, max_new_tokens=128)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(batch.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "    print(output_text)\n",
    "    return output_text\n",
    "\n",
    "def icl(video_path, processor, model):\n",
    "    conversation = [{\n",
    "        \"role\" : \"user\",\n",
    "        \"content\" : [\n",
    "        {\n",
    "            \"type\" : \"text\", \"text\" : BETTER_FORMATTED_ICL_PROMPT,\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"text\", \"text\" : \"Video:\"\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"video\", \"video\" : example_gesture_video, 'nframes' : 16\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"text\", \"text\" : COT_FIRST_EXAMPLE_ANSWER,\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"text\", \"text\" : \"Video:\"\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"video\", \"video\" : example_no_gesture_video,\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"text\", \"text\" : COT_SECOND_EXAMPLE_ANSWER, 'nframes' : 16\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"text\", \"text\" : \"Video:\"\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"video\", \"video\" : video_path, 'nframes' : 16\n",
    "        }\n",
    "        ]},\n",
    "        {\n",
    "            \"role\" : \"assistant\",\n",
    "            \"content\" : [\n",
    "                {\n",
    "                    \"type\" : \"text\", \"text\" : \"Let's think step by step: \"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    text = processor.apply_chat_template(conversation, tokenize=False, continue_final_message=True)\n",
    "    image_inputs, video_inputs = process_vision_info(conversation)\n",
    "    batch = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(model.device)\n",
    "    # print(model.device, batch.to_device())\n",
    "    input_ids = batch.input_ids\n",
    "    attention_mask = batch.attention_mask\n",
    "    generated_ids = model.generate(**batch, max_new_tokens=256)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(batch.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "    return output_text\n",
    "\n",
    "def no_format_output(video_path, processor, model):\n",
    "    conversation = [{\n",
    "        \"role\" : \"user\",\n",
    "        \"content\" : [{\n",
    "            \"type\" : \"video\",\n",
    "            \"video\" : video_path,\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"text\", \"text\" : NO_FORMAT_PROMPT,\n",
    "        },\n",
    "        \n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\" : \"assistant\",\n",
    "        \"content\" : [{\n",
    "            \n",
    "            \"type\" : \"text\", \"text\": \"Let's think step by step: \"\n",
    "\n",
    "        }]\n",
    "    }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(conversation, tokenize=False, continue_final_message=True)\n",
    "    image_inputs, video_inputs = process_vision_info(conversation)\n",
    "    batch = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(model.device)\n",
    "    # print(model.device, batch.to_device())\n",
    "    generated_ids = model.generate(**batch, max_new_tokens=128)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(batch.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "    return output_text\n",
    "\n",
    "def gesture_identifier(video_path, processor, model):\n",
    "    conversation = [{\n",
    "        \"role\" : \"user\",\n",
    "        \"content\" : [\n",
    "        {\n",
    "            \"type\" : \"text\", \"text\" : \"You are an expert gesture identifier who can always identify if a human is making a gesture or not based off their pose. For our use case, a gesture is identified as an upper body human movement. Look at this video and identify if a gesture has been made:\",\n",
    "        },\n",
    "        {\n",
    "            \"type\" : \"video\",\n",
    "            \"video\" : video_path,\n",
    "            'nframes': 16\n",
    "        },\n",
    "        \n",
    "        ]\n",
    "    },\n",
    "    # {\n",
    "    #     \"role\" : \"assistant\",\n",
    "    #     \"content\" : [{\n",
    "            \n",
    "    #         \"type\" : \"text\", \"text\": \"Let's think step by step: \"\n",
    "\n",
    "    #     }]\n",
    "    # }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    print(text)\n",
    "    image_inputs, video_inputs = process_vision_info(conversation)\n",
    "            \n",
    "    batch = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(model.device)\n",
    "    for video in video_inputs:\n",
    "        for frame in video:\n",
    "            frame = frame.to(torch.uint8)\n",
    "            frame = frame.permute(1, 2, 0)\n",
    "            image_array = frame.numpy()\n",
    "            plt.imshow(image_array)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    # print(model.device, batch.to_device())\n",
    "    generated_ids = model.generate(**batch, max_new_tokens=100, do_sample=False)\n",
    "    print(generated_ids.shape)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(batch.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0].strip()\n",
    "    print(output_text)\n",
    "    return output_text\n",
    "\n",
    "\n",
    "# print(gesture_identifier('/scratch/bjb3az/interaction/good_data/target_002_master_chef_can_:0000_iteration_4_gesture_False_2024-12-05_00:14:22_seed_65.mp4', processor, model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Self-consistency majority voting\n",
    "def self_consistency(video_path):\n",
    "\n",
    "    outputs = []\n",
    "    NUM_ITER = 5\n",
    "    for i in range(NUM_ITER):\n",
    "        out_text = icl(video_path, processor, model)\n",
    "        out_text = out_text.replace('*', '').strip()\n",
    "        outputs.append(out_text)\n",
    "    final_output = majority_voting_action(outputs)\n",
    "    return final_output\n",
    "\n",
    "# /scratch/bjb3az/interaction/good_data/target_002_master_chef_can_:0000_iteration_4_gesture_False_2024-12-05_00:14:22_seed_65.mp4\n",
    "# /scratch/bjb3az/interaction/good_data/target_002_master_chef_can_:0000_iteration_1_gesture_True_2024-11-19_01:14:56_seed_75.mp4\n",
    "# /scratch/bjb3az/interaction/good_data/target_010_potted_meat_can_:0000_iteration_30_gesture_False_2024-11-17_13:57:55_seed_65.mp4\n",
    "\n",
    "# final_action = self_consistency('/scratch/bjb3az/interaction/good_data/target_002_master_chef_can_:0000_iteration_1_gesture_True_2024-11-19_01:14:56_seed_75.mp4')\n",
    "# print(final_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_007_tuna_fish_can_:0000_iteration_8_gesture_True_.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qwen-vl-utils using decord to read video.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The human makes a visible gesture in my general direction, and it seems like they are pointing straight ahead.\n",
      "\n",
      "1. Identify Gesture: The human's gesture points directly forward, indicating movement in a straight line.\n",
      "2. Determine Intent: Based on the gesture, the human likely intends to move straight ahead towards the same destination.\n",
      "3. Avoid Collision: Since both the human and I share the same destination and the human is already moving straight ahead, maintaining my current path will align with the human’s intended route without conflict.\n",
      "\n",
      "Given these observations:\n",
      "\n",
      "### Walk Straight\n",
      "595\n",
      "1. Identify Human Gesture: \n",
      "   - The human extends his right hand toward me.\n",
      "\n",
      "2. Assess Human Intent:\n",
      "   - Extending one’s hand typically indicates offering assistance or directing someone.\n",
      "   - Since he is facing forward, his hand extended towards me suggests he wants me to stop momentarily or may want to physically guide me in some manner.\n",
      "\n",
      "3. Determine Next Action:\n",
      "   - Given his gesture, likely indicating he wishes for me to pause, it would be sensible for me to stay put at this time until further guidance is given.\n",
      "\n",
      "4. Avoid Collision:\n",
      "   - By staying still, we avoid potential physical contact resulting from misunderstanding intentions.\n",
      "\n",
      "Therefore:\n",
      "\n",
      "### Stay\n",
      "671\n",
      "The human is not currently making a visible gesture in my direction. However, the human's posture indicates a readiness to interact or pass near me.\n",
      "\n",
      "At the current moment, our paths appear likely to converge as we move forward; without intervention, this convergence could lead to a physical obstacle or inconvenience at worst, a minor collision at best.\n",
      "\n",
      "Since no direct indication has been given about how the human wishes to navigate around me, considering basic rules of pedestrian etiquette suggests yielding space to others to prevent potential discomfort or impact.\n",
      "\n",
      "Hence, pausing for a brief interval would allow the human either to adjust their own route independently or explicitly indicate a preference for one of us to proceed while the other momentarily holds position.\n",
      "\n",
      "So, the most sensible next action, grounded in the observation of the situation so far, would be:\n",
      "\n",
      "### Wait\n",
      "893\n",
      "1. Identify Gestures: The human is moving directly toward me without any visible gestures in my direction.\n",
      "\n",
      "2. Human Intent: Given his direct approach and the fact he is facing me straight-on, the human likely intends to pass directly ahead of me.\n",
      "\n",
      "3. Avoidance Strategy: Since the human's intent indicates he plans to move forward and possibly intersect with my current path, I need to adjust my route to prevent a collision while respecting the human’s intended course.\n",
      "\n",
      "4. Best Course of Action:\n",
      "\n",
      "Given these observations, the most appropriate action would be to stay put until the human has passed. Staying stationary allows the human to proceed unobstructed and avoids potential contact. Once the human passes, I can then resume my original path.\n",
      "\n",
      "Therefore, the best action at this exact point is:\n",
      "\n",
      "### Stay\n",
      "813\n",
      "1. Identify Human Gesture: The human isn't making any gesture at the moment.\n",
      "\n",
      "2. Determine Human Intent: Based on their posture and previous actions, the human seems to be moving forward without any specific direction indicated towards me.\n",
      "\n",
      "3. Current Trajectories: Both the human and I are heading towards the left area of the room, likely indicating that our destination might be near the kitchen area seen in the video.\n",
      "\n",
      "4. Avoid Collision: Given that we're headed in similar directions without a direct signal from the human to proceed differently, I'll need to adjust my route slightly to ensure safety and space. Walking straight ahead could bring me closer to the human, risking potential interference.\n",
      "\n",
      "5. Next Action:\n",
      "   - Since the human's intention isn’t explicitly directed, I should take initiative to maintain distance and ensure smooth coexistence within the shared space.\n",
      "   \n",
      "### Walk Left\n",
      "905\n",
      "1) Identifying if the human is making a gesture:\n",
      "The human is not seen making a direct gesture toward me at this moment.\n",
      "\n",
      "2) Determining Next Action to Avoid Collision Based on Gesture and Body Language:\n",
      "\n",
      "We're currently moving towards each other due to the intersecting paths within the room, which could lead to a collision. Given that no explicit gesture has been made towards me at this time, I need to infer the human’s intended route by considering his body language and previous actions.\n",
      "\n",
      "The person was initially entering the room from the door, then paused mid-action to turn around upon seeing me. Although he hasn’t directly gestured away from me yet, his pause indicates awareness of my presence and potential need to alter one of our courses.\n",
      "\n",
      "Given that we started out heading towards opposite directions originally and now find ourselves converging, the most logical assumption would be that the human might intend to either stop his current movement or redirect himself to avoid confrontation. In this context, it's safer for me to interpret his pause as a sign for me to take evasive action because I am the more agile party capable of reacting quickly without interrupting his movement significantly.\n",
      "\n",
      "3) Deciding the Course of Action:\n",
      "\n",
      "Considering these points, I should move laterally across the room rather than\n",
      "1332\n",
      "No match found.\n",
      "First, observe whether the human is making a gesture in your direction. No explicit gesture is observed.\n",
      "\n",
      "Next, assess the current situation based on observable cues:\n",
      "\n",
      "1. You are positioned behind the sofa.\n",
      "2. The human has entered the living room and is now standing near the wall opposite you.\n",
      "3. There is no clear indication of immediate conflict due to the distance separating you and the human.\n",
      "4. Given the lack of a direct gesture indicating an intended path change, combined with your current position behind the sofa, a simple adjustment may suffice.\n",
      "\n",
      "Therefore, to ensure safety while minimizing disruption to your overall navigation plan without deviating significantly from your current route:\n",
      "\n",
      "### Walk Left\n",
      "720\n",
      "First, determine if the human is making a gesture in your direction. In the video, the human enters the living area, looks around, walks back towards the couch, and then turns toward you while reaching out his hand, seemingly gesturing to you.\n",
      "\n",
      "Secondly, assess whether avoiding collisions involves walking around the object or altering your own trajectory. Since the human is extending his arm towards you, it suggests he wants you to move out of his way rather than him walking around you. Additionally, the human is moving forward, so staying in place would lead to a collision.\n",
      "\n",
      "Finally, identify how to alter your trajectory accordingly. Based on the human’s gesture and current actions, he intends for you to stop and let him pass. Therefore:\n",
      "\n",
      "### Stop\n",
      "758\n",
      "the human does not appear to be making a direct gesture towards me. However, given that we share the same destination, it is reasonable to infer his intention is to move forward.\n",
      "\n",
      "1. Identify the human’s Intent: Although no explicit gesture has been made, observing the human's forward-facing orientation and lack of hesitation suggests he aims to proceed directly ahead toward the common goal.\n",
      "\n",
      "2. Determine Trajectory Overlap: Since we're moving in parallel paths, there’s potential for collision. Preemptively adjusting course could prevent this.\n",
      "\n",
      "3. Decide Next Action Based on Human Intent: Without a specific directive, aligning actions with the shared destination while ensuring safe passage would involve either staying put temporarily or shifting slightly out of the direct line.\n",
      "\n",
      "Given these considerations:\n",
      "\n",
      "- ### Stay\n",
      "829\n",
      "1. Human Gesture: The human doesn't appear to be making a visible gesture in my direction.\n",
      "\n",
      "2. Trajectory Analysis:\n",
      "   - My current trajectory is straight ahead.\n",
      "   - The human's trajectory suggests they intend to move closer to my current position.\n",
      "\n",
      "3. Avoidance Strategy:\n",
      "   - Given that we're heading in similar directions and the human might come close to my path, I need to make a decision to avoid potential collision while ensuring we maintain our intended destinations.\n",
      "\n",
      "4. Decision Making:\n",
      "   - Since the human isn't directly obstructing my path yet, I'll prioritize maintaining distance by slightly adjusting my direction without affecting our final destination.\n",
      "   - Walking to my left would create more space between us and allow the human to pass comfortably.\n",
      "\n",
      "5. Action Plan:\n",
      "   - Adjust my path to the left to increase the gap between myself and the human.\n",
      "   - Continue observing the human's actions to ensure no further adjustments are needed.\n",
      "\n",
      "### Walk Left\n",
      "975\n",
      "defaultdict(<class 'int'>, {'Walk Straight': 1, 'Stay': 3, 'Wait': 1, 'Walk Left': 3, 'Stop': 1})\n",
      "['Stay', 'Walk Left']\n",
      "Iteration number 0, duration time 265.2672896385193\n",
      "target_009_gelatin_box_:0000_iteration_30_gesture_False_2024-11-17_13:57:55_seed_65.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     11\u001b[0m cur_video_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(video_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m data\u001b[38;5;241m.\u001b[39mappend([filename, \u001b[43mself_consistency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_video_path\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration number \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mind\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, duration time \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m, in \u001b[0;36mself_consistency\u001b[0;34m(video_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m NUM_ITER \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_ITER):\n\u001b[0;32m----> 7\u001b[0m     out_text \u001b[38;5;241m=\u001b[39m \u001b[43micl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     out_text \u001b[38;5;241m=\u001b[39m out_text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      9\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(out_text)\n",
      "Cell \u001b[0;32mIn[12], line 91\u001b[0m, in \u001b[0;36micl\u001b[0;34m(video_path, processor, model)\u001b[0m\n\u001b[1;32m     89\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m     90\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mattention_mask\n\u001b[0;32m---> 91\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m generated_ids_trimmed \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     93\u001b[0m     out_ids[\u001b[38;5;28mlen\u001b[39m(in_ids) :] \u001b[38;5;28;01mfor\u001b[39;00m in_ids, out_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch\u001b[38;5;241m.\u001b[39minput_ids, generated_ids)\n\u001b[1;32m     94\u001b[0m ]\n\u001b[1;32m     95\u001b[0m output_text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(\n\u001b[1;32m     96\u001b[0m     generated_ids_trimmed, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     97\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/scratch/bjb3az/.conda/envs/habitat/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/bjb3az/.conda/envs/habitat/lib/python3.9/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m/scratch/bjb3az/.conda/envs/habitat/lib/python3.9/site-packages/transformers/generation/utils.py:3249\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3247\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3248\u001b[0m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[0;32m-> 3249\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3251\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "data = [\n",
    "    [\"file_name\", \"model_output\"]\n",
    "]\n",
    "import time\n",
    "video_dir = os.path.join(os.getcwd(), 'good_data')\n",
    "if os.path.isdir(video_dir):\n",
    "    for ind, filename in enumerate(os.listdir(video_dir)):\n",
    "        print(filename)\n",
    "        start_time = time.time()\n",
    "        cur_video_path = os.path.join(video_dir, f\"{filename}\")\n",
    "        data.append([filename, self_consistency(cur_video_path)])\n",
    "        print(f\"Iteration number {ind}, duration time {time.time() - start_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(video_path, 'metadata.csv'), 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(data)\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "habitat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

output_dir: llava_finetune
eval_strategy: 'steps'
eval_steps: 20
# auto_find_batch_size: True
per_device_train_batch_size: 2
per_device_eval_batch_size: 4
gradient_accumulation_steps: 8
learning_rate: !!float 2e-05
lr_scheduler_type: 'cosine'
warmup_ratio: 0.1
weight_decay: 0.01
num_train_epochs: 7
logging_steps: 20
save_strategy: 'no'
save_total_limit: 1
# fp16: True
bf16: True
bf16_full_eval: True
optim: 'adamw_torch'

push_to_hub: True
hub_model_id: 'kingsleykim/llava-vl-7b-finetune-1'
report_to: 'wandb'
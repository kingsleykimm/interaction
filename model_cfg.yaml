output_dir: llava_finetune
eval_strategy: 'steps'
eval_steps: 20
auto_find_batch_size: True
# per_device_train_batch_size: 4
# per_device_eval_batch_size: 4
gradient_accumulation_steps: 4
learning_rate: !!float 2e-05
max_steps: 80
lr_scheduler_type: 'cosine'
warmup_ratio: 0.1

logging_steps: 20
save_strategy: 'no'
save_total_limit: 1
# fp16: True
bf16: True
bf16_full_eval: True
optim: 'adamw_torch'

push_to_hub: True
hub_model_id: 'kingsleykim/sn-finetune-v3'
label_names:
  - 'model_outputs'
report_to: 'wandb'
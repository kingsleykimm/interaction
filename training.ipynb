{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import av\n",
    "import fsspec\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, Seq2SeqTrainingArguments, DataCollatorForLanguageModeling\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, LlavaNextVideoForConditionalGeneration\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from huggingface_hub import snapshot_download, hf_hub_download, HfFileSystem\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 4\n",
    "NUM_FRAMES = 8 # more frames -> more VRAM needed\n",
    "DATASET_PATH = \"/scratch/bjb3az/interaction/datasets\" # path where to save the dataset\n",
    "OUTPUT_DIR = \"/scratch/bjb3az/interaction/checkpoints\" # path where to save the checkpoints\n",
    "MODEL_ID = \"llava-hf/LLaVa-NeXT-Video-7b-hf\"\n",
    "\n",
    "USE_LORA = True\n",
    "USE_QLORA = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We need to convert our video dataset into a huggingface dataset format\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
